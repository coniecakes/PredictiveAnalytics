---
title: "Exercise 7 - Dimensionality"
subtitle: "Ridge, LASSO, PCR and PLS"
author: "Place your last name here"
date: "Place your exercise date here"
output: 
  word_document:
     toc: true
     toc_depth: 2
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=T, include=T, warning=F, message=F)
```

## Preparation 

Download the **Ex7_Dimensionality_YourLastName.Rmd** R Markdown file to your working directory, rename it to include your last name and follow the instructions below. When you finish, knit it into an Word or PDF file and upload it.

Also, enter the following commend to set the Random Number Generator default:

```{r}
RNGkind(sample.kind="default")
```

## 1. Ridge Regressions

**1.1 Data Work**

In class, we fitted a Ridge regression model with a quantitative outcome, which is an alternative to OLS when there are a large number of predictors, high dimensionality and multi-collinearity. In this exercise, you will do the same, but with a Ridge Logistic regression. We will use the **Heart.csv** South African Heart Disease data we used earlier to predict **chd** (1 - coronary heart disease; 0 no heart disease). You can view the dataset documentation at: https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.info.txt

Let's start by loading the `glmnet()` package, reading the data and creating the **x predictor matrix** and **y outcome**. As mentioned in class use [,-1] in the model.matrix() function to remove the first column, which contains 1's representing the intercept, otherwise you will have 2 intercepts in the Ridge output.

```{r}
# Done for you

options(scipen=4)

library(glmnet) # Contains functions for Ridge and LASSO
heart <- read.table("Heart.csv", sep=",", head=T)

x=model.matrix(chd~.,data=heart)[,-1]
y=heart$chd
```

**1.2 Fit Ridge Model**

Fit the Ridge model with x and y, using the attribute `family="binomial"`. Name the resulting `glmnet()` object **ridge.logit** and then `plot()` it. This plot graphs the **L2 Norm** graph (incorrectly labeled as L1 Norm) showing how the coefficients shrink (towards the left) as lambda goes up (to the left). To the far right are the OLS coefficients. To the far left is the null model. Notice how none of the coefficients become 0 until lambda is infitity (far left).

Technical note: when we fit logit models with `glm()` we use `family=binomial(link="logit")`, but the `glmnet()` function is different and only requires `family="binomial"`.

```{r}

```

**1.3 Cross-Validation**

Enter `set.seed(1)` to get the same results shown in the solution. 

Then use the `cv.glmnet()` function to create a **10-Fold Cross Validation** (default) object for this model. Don't forget to include the `family="binomial"` attribute. Name this object **ridge.logit.cv10**.

Then use the `cbind()` function to list this object's **$lambda** (shrinkage) and **$cvm** (mean 2LL cross-validation). Name these two columns **"Lambda"** and **"10FCV"** respectively. Notice that the output shows 100 different lambda values, each with their respective 10FCV deviance.

Finally, `plot()` the **ridge.logit.cv10** object. Notice that the plot shows log(lambda) rather than lambda, which is for display purposes.

```{r}

```

**1.4 Tuning the Model**

In the output above, you will notice that the models are ordered from high to low shrinkage, using 100 different lambdas. Also, note that the lowest **10FCV** deviance is for **Lambda=0.04099545** (your results may deviate from mine slightly due to sampling). This means that the best Ridge model is one with very little shrinkage, pretty close to a plain Logit model. The plot also shows that the best lambda is close to 0 (i.e., most negative log(lambda)). Let's find this **Best Lambda**, but this time computationally.

Retrieve the `$lambda.min` value from **ridge.logit.cv10** and store it in an object named **ridge.best.lambda**. 

Then, find the corresponding **10FCV** for this best lambda using the `min()` function on the **ridge.logit.cv10$cvm** vector. Store this value in an object named **min.cv.ridge**. This result should provide the **10FCV** value corresponding to the **Best Lambda**. 

Finally, use the `cbind` function to display the `"Best Lambda"=ridge.best.lambda`, `"Best Log Lambda"=log(ridge.best.lambda)` (to be able to spot this value in the graph) and `"Best 10FCV" = min.cv.ridge`.

```{r}

```

**1.5 Comparing Coefficients**

Use the `coef()` function to retrieve the **Ridge Regression** coefficients from the **ridge.logit** model object for the **Best Lambda** using `s=ridge.best.lambda` and store the resulting coefficient vector in an object named **ridge.coef**.

Also, retrieve the **Plain Logit** coefficients from **ridge.logit**, this time using `s=0` and naming the vector **ridge.coef.0**. 

Now use the `cbind()` function to display 4 coefficient vectors: `ridge.coef` (log-odds, Ridge), `exp(ridge.coef)` (odds, ridge), `ridge.coef.0` (log-odds plain Logit), and `exp(ridge.coef.0)` (odd, plain Logit) and store the results in an object named **all.coefs**. Then use `colnames(all.coefs)` to name the resulting columns **Best Ridge**, **Odds**, **0-Lambda Ridge**, **Odds**. Then display **all.coefs**.

```{r}

```

**1.6 Discussion**

Do you see much difference between the Ridge coefficients for Best Lambda and plain Logit? Why do you think that is?




**1.7 Interpretation** 

Provide a brief interpretation for the **famhistPresent** (binary) in the **Ridge Regression**, both in terms of **log-odds** and **odds**. The interpretation is similar to one for a regular logit model.




## 2. LASSO Regression

**2.1 LASSO**

Repeat the entire analysis in 1 above, but this time using LASSO. But notice in the plot that some coefficients drop out of the model as lambda grows, before becoming infinity. That's the key difference with Ridge.

```{r}
# Done for you

# LASSO Logit model:
lasso.logit <- glmnet(x, y, alpha=1, family="binomial") 
plot(lasso.logit)

set.seed(1)

lasso.logit.cv10 <- cv.glmnet(x, y, alpha=1, family="binomial")
cbind("Lambda"=lasso.logit.cv10$lambda, "10FCV"=lasso.logit.cv10$cvm)
plot(lasso.logit.cv10)

lasso.best.lambda <- lasso.logit.cv10$lambda.min 
min.cv.lasso <- min(lasso.logit.cv10$cvm)

cbind("Best Lambda"=lasso.best.lambda, "Best Log Lambda"=log(lasso.best.lambda), "Best 10FCV" = min.cv.lasso)

lasso.coef <- coef(lasso.logit, s=lasso.best.lambda)
lasso.coef.0 <- coef(lasso.logit, s=0)

# Bind all coefficients

all.coefs <- cbind(lasso.coef, exp(lasso.coef), lasso.coef.0, exp(lasso.coef.0))

# Label the coefficients

colnames(all.coefs) <- c("Best LASSO", "Odds", "0-Lambda LASSO", "0dds")

# Display them

all.coefs
```

**2.2 Interpretation**

Use the `cbind()` function to display the **Best Lambda** and **lowest 10FCV** for both, the Ridge and LASSO regressions in 4 columns side by side. Label each column as shown below.

Then comment, which of the two modeling methods would you select and why?

```{r}

```

Also, you can compare the Ridge and LASSO coefficients side by side:

```{r}
# Done for you

all.coefs <- cbind(lasso.coef, lasso.coef.0, ridge.coef, ridge.coef.0)
colnames(all.coefs) <- c("Best LASSO", "0-Lambda LASSO", "Best Ridge", "0-Lambda Ridge")
all.coefs
```


## 3. Principal Components Regression (PCR)

**3.1 Fit the PCR Model**

Load the **pls** library. Then enter `set.seed(1)` to get repeatable results and fit a **PCR** regression on all the data in the **mtcars** data set to predict **mpg** using all other variables as predictors. Use the `pcr()` function with `scale=T, validation="CV"` (for 10FCV). Name this PCR model **pcr.fit**. Then display the `summary()` of **pcr.fit** and draw a validation plot with `val.type="MSEP"` (MSE of the predictor).

```{r}

```

**3.2 Optimal Number of Components**

Based on the summary and the plot, what seems to be the optimal number of components?




## 4. Partial Least Squares Regression (PLSR)

**4.1 Fit the PLSR Model**

Enter `set.seed(1)` again and repeat the analysis above, but using the `plsr()` function for **PLS** instead.

```{r}

```

**4.2 Interpretation**

Based on the summary and the plot, what seems to be the optimal number of components?




**4.3 Evaluation**

Based on the RMSE for 3 components, which method would you select, PCR or PLS? Briefly explain why.

