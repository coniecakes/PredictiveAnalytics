---
title: "Exercise 7 - Dimensionality"
subtitle: "Ridge, LASSO, PCR and PLS"
author: "O'Malley, Conie"
date: "`r Sys.Date()`"
output: 
  word_document:
     toc: true
     toc_depth: 2
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=T, include=T, warning=F, message=F)
```

## Preparation 

Download the **Ex7_Dimensionality_YourLastName.Rmd** R Markdown file to your working directory, rename it to include your last name and follow the instructions below. When you finish, knit it into an Word or PDF file and upload it.

Also, enter the following commend to set the Random Number Generator default:

```{r}
RNGkind(sample.kind="default")
```

## 1. Ridge Regressions

**1.1 Data Work**

In class, we fitted a Ridge regression model with a quantitative outcome, which is an alternative to OLS when there are a large number of predictors, high dimensionality and multi-collinearity. In this exercise, you will do the same, but with a Ridge Logistic regression. We will use the **Heart.csv** South African Heart Disease data we used earlier to predict **chd** (1 - coronary heart disease; 0 no heart disease). You can view the dataset documentation at: https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.info.txt

Let's start by loading the `glmnet()` package, reading the data and creating the **x predictor matrix** and **y outcome**. As mentioned in class use [,-1] in the model.matrix() function to remove the first column, which contains 1's representing the intercept, otherwise you will have 2 intercepts in the Ridge output.

```{r}
# Done for you

options(scipen=4)

library(glmnet) # Contains functions for Ridge and LASSO
heart <- read.table("/Users/coniecakes/Library/CloudStorage/OneDrive-Personal/001. Documents - Main/023. Programming Tools/R Studio/PredictiveAnalytics/R_Exercises/data/Heart.csv", sep=",", head=T)

x=model.matrix(chd~.,data=heart)[,-1]
y=heart$chd
```

**1.2 Fit Ridge Model**

Fit the Ridge model with x and y, using the attribute `family="binomial"`. Name the resulting `glmnet()` object **ridge.logit** and then `plot()` it. This plot graphs the **L2 Norm** graph (incorrectly labeled as L1 Norm) showing how the coefficients shrink (towards the left) as lambda goes up (to the left). To the far right are the OLS coefficients. To the far left is the null model. Notice how none of the coefficients become 0 until lambda is infitity (far left).

Technical note: when we fit logit models with `glm()` we use `family=binomial(link="logit")`, but the `glmnet()` function is different and only requires `family="binomial"`.

```{r}
ridge.logit <- glmnet::glmnet(x, y, alpha = 0, family = "binomial") # fit ridge logit model
plot(ridge.logit) # plot model
```

**1.3 Cross-Validation**

Enter `set.seed(1)` to get the same results shown in the solution. 

Then use the `cv.glmnet()` function to create a **10-Fold Cross Validation** (default) object for this model. Don't forget to include the `family="binomial"` attribute. Name this object **ridge.logit.cv10**.

Then use the `cbind()` function to list this object's **$lambda** (shrinkage) and **$cvm** (mean 2LL cross-validation). Name these two columns **"Lambda"** and **"10FCV"** respectively. Notice that the output shows 100 different lambda values, each with their respective 10FCV deviance.

Finally, `plot()` the **ridge.logit.cv10** object. Notice that the plot shows log(lambda) rather than lambda, which is for display purposes.

```{r}
set.seed(1) # assign set seed
ridge.logit.cv10 <- cv.glmnet(x, y, alpha = 0, family = "binomial") # fit cv ridge logit model
cv10 <- cbind(Lambda = ridge.logit.cv10$lambda, `10FCV` = ridge.logit.cv10$cvm) # create cv10 dataframe
plot(ridge.logit.cv10) # plot model
```

**1.4 Tuning the Model**

In the output above, you will notice that the models are ordered from high to low shrinkage, using 100 different lambdas. Also, note that the lowest **10FCV** deviance is for **Lambda=0.04099545** (your results may deviate from mine slightly due to sampling). This means that the best Ridge model is one with very little shrinkage, pretty close to a plain Logit model. The plot also shows that the best lambda is close to 0 (i.e., most negative log(lambda)). Let's find this **Best Lambda**, but this time computationally.

Retrieve the `$lambda.min` value from **ridge.logit.cv10** and store it in an object named **ridge.best.lambda**. 

Then, find the corresponding **10FCV** for this best lambda using the `min()` function on the **ridge.logit.cv10$cvm** vector. Store this value in an object named **min.cv.ridge**. This result should provide the **10FCV** value corresponding to the **Best Lambda**. 

Finally, use the `cbind` function to display the `"Best Lambda"=ridge.best.lambda`, `"Best Log Lambda"=log(ridge.best.lambda)` (to be able to spot this value in the graph) and `"Best 10FCV" = min.cv.ridge`.

```{r}
ridge.best.lambda <- ridge.logit.cv10$lambda.min # find min lambda value
ridge.best.lambda
min.cv.ridge <- min(ridge.logit.cv10$cvm) # find min 2LL average
min.cv.ridge
cbind("Best Lambda" = ridge.best.lambda, "Best Log Lambda" = log(ridge.best.lambda), "Best 10FCV" = min.cv.ridge)
```

**1.5 Comparing Coefficients**

Use the `coef()` function to retrieve the **Ridge Regression** coefficients from the **ridge.logit** model object for the **Best Lambda** using `s=ridge.best.lambda` and store the resulting coefficient vector in an object named **ridge.coef**.

Also, retrieve the **Plain Logit** coefficients from **ridge.logit**, this time using `s=0` and naming the vector **ridge.coef.0**. 

Now use the `cbind()` function to display 4 coefficient vectors: `ridge.coef` (log-odds, Ridge), `exp(ridge.coef)` (odds, ridge), `ridge.coef.0` (log-odds plain Logit), and `exp(ridge.coef.0)` (odd, plain Logit) and store the results in an object named **all.coefs**. Then use `colnames(all.coefs)` to name the resulting columns **Best Ridge**, **Odds**, **0-Lambda Ridge**, **Odds**. Then display **all.coefs**.

```{r}
ridge.coef <- coef(ridge.logit, s=ridge.best.lambda) # ridge regression coefficients for best lambda
ridge.coef.0 <- coef(ridge.logit, s=0) # plain logit ridge regression coefficients
cbind(ridge.coef, exp(ridge.coef), ridge.coef.0, exp(ridge.coef.0)) -> all.coefs # create matrix vector
colnames(all.coefs) <- c("Best Ridge", "Odds", "0-Lambda Ridge", "Odds") # assign column names
all.coefs
```

**1.6 Discussion**

Do you see much difference between the Ridge coefficients for Best Lambda and plain Logit? Why do you think that is?

#### Analysis

The Ridge regression coefficients for Best Lambda and plain Logit are quite similar, which suggests that the regularization parameter is not having a significant impact on the model. This could be because the data is not highly correlated or because the sample size is large enough to provide stable estimates of the coefficients.


**1.7 Interpretation** 

Provide a brief interpretation for the **famhistPresent** (binary) in the **Ridge Regression**, both in terms of **log-odds** and **odds**. The interpretation is similar to one for a regular logit model.

#### Analysis

The log-odds of having coronary heart disease increases by 0.766 for each unit increase in famhistPresent. This means that the odds increases by a factor of $e^.766 = 2.14$ for each unit increase in famhistPresent.


## 2. LASSO Regression

**2.1 LASSO**

Repeat the entire analysis in 1 above, but this time using LASSO. But notice in the plot that some coefficients drop out of the model as lambda grows, before becoming infinity. That's the key difference with Ridge.

```{r}
# Done for you

# LASSO Logit model:
lasso.logit <- glmnet(x, y, alpha=1, family="binomial") 
plot(lasso.logit)

set.seed(1)

lasso.logit.cv10 <- cv.glmnet(x, y, alpha=1, family="binomial")
cbind("Lambda"=lasso.logit.cv10$lambda, "10FCV"=lasso.logit.cv10$cvm)
plot(lasso.logit.cv10)

lasso.best.lambda <- lasso.logit.cv10$lambda.min 
min.cv.lasso <- min(lasso.logit.cv10$cvm)

cbind("Best Lambda"=lasso.best.lambda, "Best Log Lambda"=log(lasso.best.lambda), "Best 10FCV" = min.cv.lasso)

lasso.coef <- coef(lasso.logit, s=lasso.best.lambda)
lasso.coef.0 <- coef(lasso.logit, s=0)

# Bind all coefficients

all.coefs <- cbind(lasso.coef, exp(lasso.coef), lasso.coef.0, exp(lasso.coef.0))

# Label the coefficients

colnames(all.coefs) <- c("Best LASSO", "Odds", "0-Lambda LASSO", "0dds")

# Display them

all.coefs
```

**2.2 Interpretation**

Use the `cbind()` function to display the **Best Lambda** and **lowest 10FCV** for both, the Ridge and LASSO regressions in 4 columns side by side. Label each column as shown below.

Then comment, which of the two modeling methods would you select and why?

```{r}
cbind("Best Lambda (Ridge)" = ridge.best.lambda, "Best 10FCF (Ridge)" = min.cv.lasso, "Best Lambda (Lasso)" = lasso.best.lambda, "Lowest 10FCV (Lasso)" = min.cv.ridge)
```

#### Analysis

The cross validation results of both are very similar, with the Ridge slightly outperforming the Lasso. The Lasso has a lower best lambda value. I would select the Lasso model because it excludes the most insignificant variables, aligning with the principle of model simplicity. I would fit a model based on this and test for multicollinearity in the new model before proceeding. 


Also, you can compare the Ridge and LASSO coefficients side by side:

```{r}
# Done for you

all.coefs <- cbind(lasso.coef, lasso.coef.0, ridge.coef, ridge.coef.0)
colnames(all.coefs) <- c("Best LASSO", "0-Lambda LASSO", "Best Ridge", "0-Lambda Ridge")
all.coefs
```


## 3. Principal Components Regression (PCR)

**3.1 Fit the PCR Model**

Load the **pls** library. Then enter `set.seed(1)` to get repeatable results and fit a **PCR** regression on all the data in the **mtcars** data set to predict **mpg** using all other variables as predictors. Use the `pcr()` function with `scale=T, validation="CV"` (for 10FCV). Name this PCR model **pcr.fit**. Then display the `summary()` of **pcr.fit** and draw a validation plot with `val.type="MSEP"` (MSE of the predictor).

```{r}
library(pls)
set.seed(1)
pcr.fit <- pcr(mpg ~ ., data=mtcars, scale=T, validation="CV")
summary(pcr.fit)
validationplot(pcr.fit, val.type="MSEP")
```

**3.2 Optimal Number of Components**

Based on the summary and the plot, what seems to be the optimal number of components?

#### Analysis

There is a clear elbow at 1 component and a subtle elbow at 3. After reviewing the RMSE and explained variance, the 3 component model exceeds the 1 component model because it has a lower RMSE (2.610 < 2.763) and has significant explanatory power for the x variables (90.07) and response variable (85.40).


## 4. Partial Least Squares Regression (PLSR)

**4.1 Fit the PLSR Model**

Enter `set.seed(1)` again and repeat the analysis above, but using the `plsr()` function for **PLS** instead.

```{r}
set.seed(1)
plsr.fit <- plsr(mpg ~ ., data=mtcars, scale=T, validation="CV")
summary(plsr.fit)
validationplot(plsr.fit, val.type="MSEP")
```

**4.2 Interpretation**

Based on the summary and the plot, what seems to be the optimal number of components?

#### Analysis

There is again a clear elbow at 1 component, but after revieiwing the RMSE and explained variance, the 3 component model exceeds the 1 component model because it has significanly greater explanatory power for the x variables (89.94) and response variable (85.75). The 1 componenet model, while having a smaller RMSE (2.750 < 2.736), has a much lower explained variance of x (57.58 < 89.94). Therefore, I would select the 3 component model because it has a higher explained variance and is more interpretable than the 1 component model.


**4.3 Evaluation**

Based on the RMSE for 3 components, which method would you select, PCR or PLS? Briefly explain why.

#### Analysis

I would select the 3 Component PCR becase it has the lowest RMSE (2.610) and highest explanatory value of x and the response variable (90.07 & 85.40, respectively).